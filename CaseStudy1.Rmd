---
title: "Case Study 1 - Beers and Breweries in USA"
author: "Yucheol Shin, Tai Chowdhury, Patricia Attah"
date: "6/21/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prepare Data
We first prepare data. We import Brew and Breweries data from CSV files and include necessary libraries for our code.
```{r}
options(warn=-1)
library(ggplot2)
library(tidyr)
library(plyr)
library(dplyr)
library(class)
library(caret)
library(e1071)
library("RColorBrewer")

beers = read.csv("C:/Users/User 1/Desktop/SMU/DS6306/DS6306-GroupProject1/data/Beers.csv", header = TRUE)
breweries = read.csv("C:/Users/User 1/Desktop/SMU/DS6306/DS6306-GroupProject1/data/Breweries.csv", header = TRUE)
```

# Number of Breweries per state
We want to analyze number of breweries in each states. We count the number of breweries for each state and plot it on top of each bar. 
From the graph, we can see that Colorado and California have most number of breweries. We see that some states have only 1 breweries such as DC, North and South Dakota. From this graph, we can ask a question why some states have more breweries than others.
```{r}
totalState = count(breweries, State)
breweries %>% arrange() %>% ggplot(aes(x=State, fill = State)) + geom_bar() + geom_text(aes(State, n + 1, label=n, fill = NULL), data= totalState) + ggtitle("Number of Breweries for each State") + theme(axis.text.x = element_text(angle=90, vjust=0.6))
```

# Merge two data
Breweries and Beer data are two separate data. Merging these two datas will give more variables to analyze. For example, we can look into the relationship between states and beers.
In order to merge, we need to find if they have key variable that we can join together. Breweries data has Brew_ID and Beer data has Brewery_id which we can merge. Converting the name of column in Beer, two datas are merged as below.
```{r}
colnames(beers)[5] = "Brew_ID"
fullData = merge(beers, breweries, by = "Brew_ID")
head(fullData)
```

# Missing values
In order to process the analysis, we need to clean up data as there might be some missing data or incorrectly formatted data. Below are the code that we have ran to find out if there is any missing data. 
```{r}
sapply(fullData, function(x) sum(is.na(x)))

cleanData = fullData %>% filter(!is.na(ABV) & !is.na(IBU))
```
There are 1005 rows of data that do not have IBU value. We need a IBU data in order to make an analysis so we decided to drop the rows that are missing IBU and ABV data. 

# Median ABV and IBU per states
We want to look at the median values for each state. In order to get median for each states, we collecte the data grouping by state and summarize them. With the data we calculated, we draw a bar charts with median ABV and IBU of all states. 
```{r}
cleanData %>% group_by(State) %>% summarize(medianABV = median(ABV), medianIBU = median(IBU), count = n())

cleanData %>% 
  group_by(State) %>% 
  summarise(medianABV = median(ABV)) %>% 
  gather(key, value, -State) %>% 
  ggplot(aes(State, value, fill = key)) + geom_bar(stat = "identity", position = "dodge") + ggtitle("Median ABV for each State") + theme(axis.text.x = element_text(angle=90, vjust=0.6))

cleanData %>% 
  group_by(State) %>% 
  summarise(medianIBU = median(IBU)) %>% 
  gather(key, value, -State) %>% 
  ggplot(aes(State, value, fill = key)) + geom_bar(stat = "identity", position = "dodge") + ggtitle("Median IBU for each State") + theme(axis.text.x = element_text(angle=90, vjust=0.6))

```

```{r}
medABV <- cleanData %>% group_by(State) %>% summarize(medianABV = median(ABV), medianIBU = median(IBU), count = n()) %>% arrange(-medianABV)
medABV <- medABV %>% select(State, medianABV)

medIBU <- cleanData %>% group_by(State) %>% summarize(medianABV = median(ABV), medianIBU = median(IBU), count = n()) %>% arrange(-medianIBU)
medIBU <- medIBU %>% select(State, medianIBU)

head(medABV)
tail(medABV)

head(medIBU)
tail(medIBU)
```
In median ABV bar chart, we can see it quite evenly spread out except for Arizona and Utah that it has significatly lower medians than others. We see that Maine and Colorado has much higher median ABV than other states.
For median IBU bar chart, results come out to be distributed wider than median ABV. We see there is dramatic differences for each states. We found Maine and West Virgina have highest median IBU, and Kansas and Wisconsin have lowest median IBU.

# Max ABV and IBU of state
In order to get the max ABV and IBU value, we followed two approaches. One is to get max values for each state by grouping state and summarizing each state. Another appropach is to get max values among all states. 
```{r}
cleanData %>% group_by(State) %>% summarize(maxABV = max(ABV), maxIBU = max(IBU))

maxABV = max(cleanData$ABV)
maxIBU = max(cleanData$IBU)

cleanData %>% filter(ABV == maxABV)
cleanData %>% filter(IBU == maxIBU)

```
First chart display max ABV and IBU for each state. From the data, we found that London Balling has ABV of 0.125 and it has maximum ABV among all beers. We also found Bitter Bitch Imperial IPA contains IBU of 138 which is the maximum among all beers.



# Summarize ABV
Checking distribution of the data is one of key part of EDA. We plot several distributions graphs of ABV to check its normality.  
```{r}
summary(cleanData$ABV)

# Histogram of ABV Percentage 
cleanData  %>% ggplot(aes(ABV*100)) + geom_histogram(fill="darkblue",color="black", binwidth= 0.375) + xlab("% Alcohol by Volume (%ABV)") + ggtitle("Distribution of Beer %ABV, Right-Skewed") 

# Box Plot
boxplot(cleanData$ABV, col='orange',main = 'Alcohol by volume')

# QQ plot for normality check
qqnorm(cleanData$ABV, pch = 1, frame = FALSE)
qqline(cleanData$ABV, col = "steelblue", lwd = 2, main = 'Alcohol by volume')

```
We see it's quite right skewed from its histogram. QQ plot also showed that this is not normally distributed data as it has some curve at upper quantiles. In the box plot, we clearly see there are some outliers. such as London Balling beer we found from MAX ABV. 

# Relationship between ABV and IBU
We made scatter plot between ABV and IBU to see what is the relationship, and we see that it has some positive relationship that as ABV Value goes up IBU tend to go up as well. 
```{r}
# Scatter Plot for ABV vs IBU for each State
cleanData %>% ggplot(aes(x=IBU, y=ABV, color=State)) + geom_point()

#7 Scatter Plot for ABV vs IBU relationship
theme_set(theme_bw())  # pre-set the bw theme.
g <- ggplot(cleanData, aes(IBU, ABV, color='red'))
g + geom_point(color='blue') + 
  geom_smooth(method="lm", se=F) +
  labs(y="ABV", 
       x="IBU", 
       title="ABV vs IBU Relationship")

```


# KNN
We would like to build a classification model using K-Nearest-Neighborhood method to identify which beer style it predicts based on ABV and IBU. We first clean up our data to have only Ale and IPA, so we modified the data.

We used external cross validation that we split out dataset that 70% of data is randomly assigned to train dataset and 30% of data is randomly assigned to test dataset. Then we ran KNN model to get the prediction result.
```{r}
set.seed(4)
filterData = cleanData %>% filter(grepl("Ale", Style) | grepl("IPA", Style))
filterData$Style[grepl("Ale", filterData$Style)] = "Ale"
filterData$Style[grepl("IPA", filterData$Style)] = "IPA"
filterData$Style = as.factor(filterData$Style)

splitPerc = .70
trainIndices = sample(1:dim(filterData)[1],round(splitPerc * dim(filterData)[1]))
train = filterData[trainIndices,]
test = filterData[-trainIndices,]

classifications = knn(train[,c(4,5)],test[,c(4,5)],train$Style, prob = TRUE, k = 5)
table(test$Style,classifications)
CM = confusionMatrix(table(test$Style,classifications))
CM

```
From the confusion matrix, we can see our model provide accuracy of 0.8339. Looking at sensitivity, predicting right Ale, it has 0.8778 which is quite good accuracy. One of the intesting observation is that our model has predicted IPA correctly. The number came out to be bit low, which is 0.7573. We can ask a question here why predicting IPA is more challenging than ALE.


# KNN Acuracty Check
The accuracy is different depending on K value. In order to get the best result, we need to give right k value for out model. To resolve this, we ran 90 times with different k values. From the accuracy graph, we found k=6 provides us best accuracy result.
```{r}
set.seed(4)
accs = data.frame(accuracy = numeric(90), k = numeric(90))
highAcc = 0
highK = 0
for(i in 1:90)
{
  classifications = knn(train[,c(4,5)],test[,c(4,5)],train$Style, prob = TRUE, k = i)
  table(test$Style,classifications)
  CM = confusionMatrix(table(test$Style,classifications))
  accs$accuracy[i] = CM$overall[1]
  accs$k[i] = i
  if(highAcc < accs$accuracy[i]) {
    highAcc = accs$accuracy[i]
    highK = i
  }
}
plot(accs$k,accs$accuracy, type = "l", xlab = "k")
highK
```

# KNN Cluster plot with k=3
We made a cluster plot to visually inspect our result of prediction. In this plot, red color represents Ale and blue color represents IPA. Then bigger dots represent the prediction of beer style and smaller dots represent the actual style of beers. When there is two contrast colors on a same dot, it indicats that this data point has incorrect prediction.
```{r}
set.seed(4)
fit = knn(train[,c(4,5)],test[,c(4,5)],train$Style, k=3)

k3DF = data.frame(test, predicted = fit)

k3DFBoundary = data.frame(x = k3DF$ABV, 
                      y = k3DF$IBU, 
                      predicted = k3DF$predicted)

find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(k3DFBoundary, .variables = "predicted", .fun = find_hull)

test$Actual = as.character(test$Style)
test$Actual = paste("Actual_", test$Actual)

legends = c("Prediction" = 16, "Actual" = 17)

ggplot() + 
  geom_point(data=k3DF,aes(ABV, IBU, color=predicted, fill=predicted), size = 5) + 
  geom_polygon(data = boundary, aes(x,y, color=predicted, fill=predicted), alpha = 0.5)+
  geom_point(aes(ABV, IBU, color=Style), data=test) + ggtitle("KNN Cluster Plot with k = 3")
```

# KNN Cluster plot with k=6
```{r}
set.seed(4)
fit = knn(train[,c(4,5)],test[,c(4,5)],train$Style, k=6)

k6DF = data.frame(test, predicted = fit)

k6DFBoundary = data.frame(x = k6DF$ABV, 
                      y = k6DF$IBU, 
                      predicted = k6DF$predicted)

find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(k6DFBoundary, .variables = "predicted", .fun = find_hull)

test$Actual = as.character(test$Style)
test$Actual = paste("Actual_", test$Actual)

ggplot() + 
  geom_point(data=k6DF,aes(ABV, IBU, color=predicted, fill=predicted), size = 5) + 
  geom_polygon(data = boundary, aes(x,y, color=predicted, fill=predicted), alpha = 0.5)+
  geom_point(aes(ABV, IBU, color=Style), data=test) + ggtitle("KNN Cluster Plot with k = 6") 
```
We have two plots to show using right K value create better prediction. As you can see the plot with k as 6, the one that has provided high accuracy, has provided more accurate data points on the cluster than plot with k as 3

We can make several assumption from this plot. We can see there is cluster of IPA on upper plot. Thus we can assume that beers are mostly like to be IPA when it has high IBU. Similarly looking at the cluster on the bottom, we can make assumption that beers are most likely to be an Ale when it has lower IBU. ABV can be also another factor of determining the style of beers as we can see there are more Ale on lower ABV then higher ABV and more IPA on higher ABV than lower ABV.

We can make another assumption from incorrect prediction occured middle area where IBU is between 30 to 60 and ABV is between 0.04 to 0.08. We see quite good amount of inaccurate prediction resulted. From this zone, where predictions are mixed, we can make an assumption that IBU from 30 to 60 and ABV from 0.04 to 0.08. Thus in order to get a better prediction we require additional features to get better prediction such as malt and hop ratio or types of ingreditient etc.


# KNN Cluster Plot for American Ales
From the KNN cluster plot of Ale vs IPA, we found beers with high ABV and IBU are most likely IPA and beers with low ABV and IBU are most likely Ale. We want to know why there are some Ales on upper side of plot and IPAs on low IBU/ABV. To get in deeper, we made a KNN cluster plot just for Ale, especially American to reduce number of variables. 
```{r}
set.seed(4)
splitPerc = .70
aleData = cleanData %>% filter(grepl("Ale", Style) & grepl("American", Style))
trainIndices = sample(1:dim(aleData)[1],round(splitPerc * dim(aleData)[1]))
trainAle = aleData[trainIndices,]
testAle = aleData[-trainIndices,]
fit = knn(trainAle[,c(4,5)],testAle[,c(4,5)],trainAle$Style, k=6)

predAleDF = data.frame(testAle, predicted = fit)

predAleBoundary = data.frame(x = predAleDF$ABV, 
                      y = predAleDF$IBU, 
                      predicted = predAleDF$predicted)

find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(predAleBoundary, .variables = "predicted", .fun = find_hull)

palettes = brewer.pal(n = 9, name = "Set3")
colors = c("American Amber / Red Ale" = palettes[1], "American Black Ale" = palettes[2], "American Blonde Ale" = palettes[3], "American Brown Ale" = palettes[4], "American Dark Wheat Ale" = palettes[5], "American Pale Ale (APA)" = palettes[6], "American Pale Wheat Ale" = palettes[7], "American Strong Ale" = palettes[8], "American Wild Ale" = palettes[9])
ggplot() + 
  geom_point(data=predAleDF,aes(ABV, IBU, color=predicted, fill=predicted), size = 5) + 
  geom_polygon(data = boundary, aes(x,y, color=predicted, fill=predicted), alpha = 0.5)+
  geom_point(aes(ABV, IBU, color=Style), data=testAle) + ggtitle("KNN Cluster Plot for American Ales") +     scale_color_manual(values = colors) + scale_fill_manual(values = colors)
```
Looking at the plot, we see the some types of Ale have high IBU and ABV even though it's not an IPA. American black Ale is one the type, which sometimes called as Black IPA. If we set this type as IPA, we could have gotten better classfication results than before.

# KNN Cluster Plot for IPA
We made another KNN cluster plot just for IPA. 
```{r}
set.seed(4)
splitPerc = .70
ipaData  = cleanData %>% filter(grepl("IPA", Style))
trainIndices = sample(1:dim(ipaData)[1],round(splitPerc * dim(ipaData)[1]))
trainIPA = ipaData[trainIndices,]
testIPA = ipaData[-trainIndices,]
fit = knn(trainIPA[,c(4,5)],testIPA[,c(4,5)],trainIPA$Style, k=6)
predIpaDF = data.frame(testIPA, predicted = fit)

predIpaBoundary = data.frame(x = predIpaDF$ABV, 
                      y = predIpaDF$IBU, 
                      predicted = predIpaDF$predicted)

find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(predIpaBoundary, .variables = "predicted", .fun = find_hull)

colors = c("American Double / Imperial IPA" = "red", "American IPA" = "blue", "Belgian IPA" = "green", "English India Pale Ale (IPA)" = "yellow")
ggplot() + 
  geom_point(data=predIpaDF, aes(ABV, IBU, color=predicted, fill=predicted), size = 5) + 
  geom_polygon(data = boundary, aes(x,y, color=predicted, fill=predicted), alpha = 0.5)+
  geom_point(aes(ABV, IBU, color=Style), data=testIPA) + ggtitle("KNN Cluster Plot") + ggtitle("KNN Cluster Plot for IPA") +     scale_color_manual(values = colors) + scale_fill_manual(values = colors)

```
We found American IPA have much broader range of IBU and ABV that they contains. With just two information, IBU and ABV, it is not enough to understand the relationship of ABV and IBU against its style. For IPA, we may need additional feature variables such as hop ratio or type of ingredients to get better classification model.

# State comparison for HighABV&IBU ALE vs. Low ABV&IBU IPA
We want to know which particular states have high ABV and IBU Ale and low ABV and IBU IPA. These types of Ales and IPAs are quite special cases as they don't follow the classification prediction we made. 
```{r}
filterData$IBU = as.numeric(filterData$IBU)
stateData = filterData %>% filter(((ABV<0.055) & (IBU<50) & (Style=="IPA")) | ((ABV>0.055) & (IBU>50) & (Style=="Ale")))
stateData %>% ggplot(aes(x=State, fill = Style)) + geom_bar() + ggtitle("Histogram of High ABV/IBU Ale vs. Low ABV/IBU IPA for each states") 
filterData %>% ggplot(aes(x=State, fill=Style)) +geom_bar() + ggtitle("Histogram of Ale vs. IPA for each states")
```
From the above chart, we found that Colarado and Orgeon have more high ABV and IBU Ales than other states. Thus we can make a question that why these states have more Ales that have high bitterness and alcohol level.

